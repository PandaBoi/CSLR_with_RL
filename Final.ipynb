{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PandaBoi/CSLR_with_RL/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxLGZCGgioEj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v04elr6viozQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiH4iduxVcnJ"
      },
      "source": [
        "# Import All Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjpffKpc1vSn"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "import torchvision.transforms as T\n",
        "from torch.autograd import Variable\n",
        "import pdb\n",
        "from itertools import groupby\n",
        "!pip install easydict\n",
        "from easydict import EasyDict as ED\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#maybe will need for later\n",
        "# !pip install git+https://github.com/enhuiz/phoenix-datasets\n",
        "# !pip install xmltodict\n",
        "# from phoenix_datasets import PhoenixVideoTextDataset\n",
        " \n",
        "# from torch.utils.data import DataLoader\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enxY9YG-VnKg"
      },
      "source": [
        "# 3D-ResNet Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGzEbRhxBANQ"
      },
      "source": [
        "\n",
        "def get_inplanes():\n",
        "    return [64, 128, 256, 512]\n",
        "\n",
        "\n",
        "def conv3x3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv3d(in_planes,\n",
        "                     out_planes,\n",
        "                     kernel_size=3,\n",
        "                     stride=stride,\n",
        "                     padding=1,\n",
        "                     bias=False)\n",
        "\n",
        "\n",
        "def conv1x1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv3d(in_planes,\n",
        "                     out_planes,\n",
        "                     kernel_size=1,\n",
        "                     stride=stride,\n",
        "                     bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv1x1x1(in_planes, planes)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 block,\n",
        "                 layers,\n",
        "                 block_inplanes,\n",
        "                 n_input_channels=3,\n",
        "                 conv1_t_size=7,\n",
        "                 conv1_t_stride=1,\n",
        "                 no_max_pool=False,\n",
        "                 shortcut_type='B',\n",
        "                 widen_factor=1.0,\n",
        "                 n_classes=400):\n",
        "        super().__init__()\n",
        "\n",
        "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
        "\n",
        "        self.in_planes = block_inplanes[0]\n",
        "        self.no_max_pool = no_max_pool\n",
        "\n",
        "        self.conv1 = nn.Conv3d(n_input_channels,\n",
        "                               self.in_planes,\n",
        "                               kernel_size=(conv1_t_size, 7, 7),\n",
        "                               stride=(conv1_t_stride, 2, 2),\n",
        "                               padding=(conv1_t_size // 2, 3, 3),\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
        "                                       shortcut_type)\n",
        "        self.layer2 = self._make_layer(block,\n",
        "                                       block_inplanes[1],\n",
        "                                       layers[1],\n",
        "                                       shortcut_type,\n",
        "                                       stride=2)\n",
        "        self.layer3 = self._make_layer(block,\n",
        "                                       block_inplanes[2],\n",
        "                                       layers[2],\n",
        "                                       shortcut_type,\n",
        "                                       stride=2)\n",
        "        self.layer4 = self._make_layer(block,\n",
        "                                       block_inplanes[3],\n",
        "                                       layers[3],\n",
        "                                       shortcut_type,\n",
        "                                       stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight,\n",
        "                                        mode='fan_out',\n",
        "                                        nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _downsample_basic_block(self, x, planes, stride):\n",
        "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
        "                                out.size(3), out.size(4))\n",
        "        if isinstance(out.data, torch.FloatTensor.to(device)):\n",
        "            zero_pads = zero_pads.to(device)\n",
        "\n",
        "        out = torch.cat([out.data, zero_pads], dim=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(self._downsample_basic_block,\n",
        "                                     planes=planes * block.expansion,\n",
        "                                     stride=stride)\n",
        "            else:\n",
        "\n",
        "# # agent = SC_REINFORCE(opts) # add args \n",
        "\n",
        "# # env = Environment(train_dataset, train_loader)\n",
        "â€¦#   #   if not os.path.exists(os.path.join(BASE_DIR,'Logs/')):\n",
        "#   #     os.makedirs(os.path.join(BASE_DIR,'Logs/'))\n",
        "#   #   torch.save(agent.model.state_dict(), os.path.join(BASE_DIR,'Logs/trial.pt'))\n",
        "#   # train_wers.append(1 - reward_s[-1])    \n",
        "#   # reward_a, reward_s, log_probs = np.array(reward_a), np.array(reward_s), np.array(log_probs)\n",
        "\n",
        "# a = next(iter(train_loader))\n",
        "# print(a['data'].shape, a['gloss'].shape)\n",
        "                downsample = nn.Sequential(\n",
        "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
        "                    nn.BatchNorm3d(planes * block.expansion))\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(in_planes=self.in_planes,\n",
        "                  planes=planes,\n",
        "                  stride=stride,\n",
        "                  downsample=downsample))\n",
        "        self.in_planes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.in_planes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        if not self.no_max_pool:\n",
        "            x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def generate_model(model_depth, **kwargs):\n",
        "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
        "\n",
        "    if model_depth == 10:\n",
        "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 18:\n",
        "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 34:\n",
        "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 50:\n",
        "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 101:\n",
        "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 152:\n",
        "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 200:\n",
        "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc28CVBZVwrg"
      },
      "source": [
        "# Transformer Implementation\n",
        "\n",
        "`PostionalEncoding` is a helper function which is used originally in the paper \"Attention is All you Need\". However for simplicity, we have used a single embedding layer in our implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKWAXsQux31p"
      },
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd5UHz3LA6Fu"
      },
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, heads, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            # print(energy.shape,mask.shape)\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "    ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size,)\n",
        "        self.hacky = nn.Linear(1,embed_size)\n",
        "        # self.position_embedding = PositionalEncoding(d_model = embed_size, max_len = max_length, dropout = 0.0)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def hacky_embedding(self, x):\n",
        "      total_out = []\n",
        "      for i in range(x.shape[1]):\n",
        "        inp = x[:,i]\n",
        "        out = self.hacky(inp.view(x.shape[0],-1))\n",
        "        # print(out.shape)\n",
        "        total_out.append(out)\n",
        "      total_out = torch.stack(total_out).movedim((0,1,2),(1,0,2))\n",
        "      # print(total_out.shape)\n",
        "      return total_out\n",
        "\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        out = self.dropout(\n",
        "              self.hacky_embedding(x.float())\n",
        "              # self.word_embedding(x) \n",
        "            + self.position_embedding(positions)\n",
        "        )\n",
        "\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\n",
        "        # decoder this will change. This might look a bit odd in this case.\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, None)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "        # self.position_embedding = PositionalEncoding(d_model = embed_size, max_len = max_length, dropout = 0.0)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "        # x = self.dropout((self.position_embedding(self.word_embedding(x))))\n",
        "        # x = self.dropout(x+ self.position(position_embedding(positions)))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0.3,\n",
        "        device=\"cpu\",\n",
        "        max_length=100,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        # out = F.softmax(out,-1)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnpjdXLeWW5e"
      },
      "source": [
        "# Dataset and DataLoader\n",
        "\n",
        "PyTorch provides a way to handle all the data in a class format. The data is then automatically converted to Tensors while iterations.\n",
        "\n",
        "* `PhoenixVideo`: creates a PyTorch dataset, this loads the features and labels. It also holds helper features which help in word2index and index2word conversion.\n",
        "\n",
        "* `Voc`: stores all the words and converts it to a dictionary where keys and values give us a way to convert words and index into each other.\n",
        "\n",
        "* `caffeFeatureLoader`: loads all the pre-trained features.\n",
        "\n",
        "Code inspired from :- [Dialated SLR](https://github.com/ustc-slr/DilatedSLR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n62JQwuv58lo"
      },
      "source": [
        "import os\n",
        "import struct\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "\n",
        "class PhoenixVideo(Dataset):\n",
        "    def __init__(self, corpus_dir, ANNOT_DIR, video_path, phase, DEBUG=False):\n",
        "        self.ANNOT_DIR = ANNOT_DIR\n",
        "        self.vocab_file = os.path.join(ANNOT_DIR,'automatic/newtrainingClasses.txt')\n",
        "        self.image_type = 'png'\n",
        "        self.max_video_len = 10000\n",
        "        self.corpus_dir = corpus_dir\n",
        "        self.video_path = video_path\n",
        "        self.phase = phase\n",
        "        self.alignment = {}\n",
        "        self.voc = Voc(self.vocab_file)\n",
        "\n",
        "        self.phoenix_dataset = self.load_video_list()\n",
        "        # print(self.phoenix_dataset)\n",
        "        self.data_dict = self.phoenix_dataset[phase]\n",
        "        if DEBUG == True:\n",
        "            self.data_dict = self.data_dict[:101]\n",
        "        logging.info('[DATASET: {:s}]: total {:d} samples.'.format(phase, len(self.data_dict)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_dict)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cur_vid_info = self.data_dict[idx]\n",
        "        id = cur_vid_info['id']\n",
        "        video_file = cur_vid_info['path']\n",
        "        label = cur_vid_info['label']\n",
        "        label_text = cur_vid_info['label_text']\n",
        "        video_tensor = self.load_video(video_file)\n",
        "        sample = {'id': id, 'data': video_tensor, 'label': label, 'label_text':label_text}\n",
        "        if self.phase == 'train':\n",
        "          glosses = cur_vid_info['gloss'].split(' ')\n",
        "          # print(\"The shape of glosses is\", np.shape(glosses))\n",
        "          # print(\"The shape of data is\", np.shape(video_tensor))\n",
        "          # print(\"The shape of label is\", len(label))\n",
        "          # print(\"The shape of label_text is\", len(label_text))\n",
        "          sample['gloss'], sample['gloss_text'] = self.clip_glosses(sample, glosses)\n",
        "        return sample # final return (data: (num_clips,512), label:(1,len_of_sentence), gloss: (num_clips,10), id:(1))\n",
        "\n",
        "    def clip_glosses(self, sample, glosses, window_size = 8, skip_size = 4):\n",
        "      #input shape: (1,num_of_frames_in_video)\n",
        "      # segment gloss sentence into clip sizes (num_clips,8) [assert len(data) == num_clips]\n",
        "      # add BOS and EOS to each glossclip (num_clips, 10)\n",
        "      gloss_text_clips = []\n",
        "      gloss_clips = []\n",
        "      j = 0\n",
        "      for i in range(0, len(glosses) - window_size, skip_size):\n",
        "        j = j + 1\n",
        "        gloss_clip = []\n",
        "        gloss_clip.append('<BOS>')\n",
        "        gloss_clip.extend(glosses[i:i+window_size])\n",
        "        gloss_clip.append('<EOS>')\n",
        "        gloss_clip = [] + gloss_clip\n",
        "        gloss_clips.append(self.sentence2index(gloss_clip))\n",
        "        gloss_text_clips.append(gloss_clip)\n",
        "      gloss_clips = np.array(gloss_clips)\n",
        "      # gloss_text_clips = np.array(gloss_text_clips)\n",
        "      # print(\"Len of gloss clips is\", np.shape(gloss_clips))\n",
        "      # print(\"Shape of data is\", sample['data'].shape)\n",
        "      try:\n",
        "        assert sample['data'].shape[0] == np.shape(gloss_clips)[0]\n",
        "      except AssertionError:\n",
        "        print(\"name: \",sample['id'])\n",
        "        print(\"sample and gloss shape dont match: \",sample['data'].shape[0], np.shape(gloss_clips)[0])\n",
        "      return gloss_clips, gloss_text_clips\n",
        "\n",
        "    \n",
        "    def load_video(self, video_name):\n",
        "        # print('video name: ',video_name)\n",
        "        feat = caffeFeatureLoader.loadVideoC3DFeature(video_name, 'pool5')\n",
        "        feat = torch.tensor(feat)\n",
        "        return feat\n",
        "\n",
        "    def load_video1(self, video_name):\n",
        "        frames_list = glob.glob(os.path.join(video_name, '*.{:s}'.format(self.image_type)))\n",
        "        frames_list.sort()\n",
        "        num_frame = len(frames_list)\n",
        "        if self.phase=='train' and self.sample and num_frame > self.max_video_len:\n",
        "            for _ in range(num_frame-self.max_video_len):\n",
        "                frames_list.pop(np.random.randint(len(frames_list)))\n",
        "        frames_tensor_list = [self.load_image(frame_file) for frame_file in frames_list]\n",
        "        video_tensor = torch.stack(frames_tensor_list, dim=0)\n",
        "        return video_tensor\n",
        "\n",
        "    def load_image(self, img_name):\n",
        "        image = Image.open(img_name)\n",
        "        image = self.transform(image)\n",
        "        return image\n",
        "    \n",
        "    def load_gloss(self): # TO DO using:- https://github.com/enhuiz/phoenix-datasets/blob/570481bf03a46555ca219f79ace1a2cfab149f8c/phoenix_datasets/corpora.py#L35\n",
        "        gloss_path = os.path.join(self.ANNOT_DIR,'automatic/train.alignment')\n",
        "        read = partial(pd.read_csv, sep=\" \", na_filter=False)\n",
        "        ali = read(gloss_path, header=None, names=[\"id\", \"classlabel\"])\n",
        "        cls = read(os.path.join(self.ANNOT_DIR,\"automatic/trainingClasses.txt\"))\n",
        "        # print(cls)\n",
        "        df = pd.merge(ali, cls, how=\"left\", on=\"classlabel\")\n",
        "        # print(df)\n",
        "        del df[\"classlabel\"]\n",
        "        # print(df[\"signstate\"])\n",
        "        df[\"gloss\"] = df[\"signstate\"].apply(lambda s: s.rstrip(\"012\"))\n",
        "\n",
        "        df[\"id\"] = df[\"id\"].apply(lambda s: \"/\".join(s.split(\"/\")[3:-2]))\n",
        "        grouped = df.groupby(\"id\")\n",
        "\n",
        "        gdf = grouped[\"gloss\"].agg(\" \".join)\n",
        "        sdf = grouped[\"signstate\"].agg(\" \".join)\n",
        "\n",
        "        df = pd.merge(gdf, sdf, \"inner\", \"id\")\n",
        "        # print(df.loc['01April_2010_Thursday_heute_default-0'])\n",
        "        return df\n",
        "\n",
        "\n",
        "    def load_video_list(self):\n",
        "        phoenix_dataset = {}\n",
        "        outliers = ['13April_2011_Wednesday_tagesschau_default-14'] # '05July_2010_Monday_heute_default-8'\n",
        "        for task in ['train', 'dev', 'test']:\n",
        "            if task != self.phase:\n",
        "                continue\n",
        "            dataset_path = os.path.join(self.video_path, task)\n",
        "            corpus = pd.read_csv(os.path.join(self.corpus_dir, '{:s}.corpus.csv'.format(task)), sep='|')\n",
        "            videonames = corpus['folder'].values\n",
        "            annotation = corpus['annotation'].values\n",
        "            if self.phase == 'train':\n",
        "              glosses_df = self.load_gloss()\n",
        "              # print(glosses_df.columns)\n",
        "            ids = corpus['id'].values\n",
        "            num_sample = len(ids)\n",
        "            # print('num_sample: ',num_sample)\n",
        "            video_infos = []\n",
        "            for i in range(num_sample):\n",
        "                if ids[i] in outliers:\n",
        "                    continue\n",
        "                tmp_info = {\n",
        "                    'id': ids[i],\n",
        "                    'path': os.path.join(self.video_path, task, videonames[i].replace('*.png', '')),\n",
        "                    'label_text': annotation[i],\n",
        "                    'label': np.array(self.sentence2index(annotation[i].split(' ')))\n",
        "                }\n",
        "                if self.phase == 'train':\n",
        "                  try:\n",
        "                    tmp_info['gloss'] = glosses_df.loc[ids[i]]['gloss']\n",
        "                  except:\n",
        "                    # print('pass')\n",
        "                    continue\n",
        "                video_infos.append(tmp_info)\n",
        "            phoenix_dataset[task] = video_infos\n",
        "        return phoenix_dataset\n",
        "\n",
        "    def sentence2index(self, sent):\n",
        "        #sent = sent.split(' ')\n",
        "        # print(sent)\n",
        "        s = []\n",
        "        for word in sent:\n",
        "            if word in self.voc.word2index:\n",
        "                s.append(self.voc.word2index[word])\n",
        "            else:\n",
        "                s.append(self.voc.word2index['<UNK>'])\n",
        "        return s\n",
        "\n",
        "    def index2sentence(self, indices):\n",
        "      # print(indices)\n",
        "      return [self.voc.index2word[ind] for ind in indices]\n",
        "\n",
        "\n",
        "class Voc():\n",
        "    def __init__(self, vocab_file):\n",
        "        PAD_token = 0\n",
        "        self.vocab_file = vocab_file\n",
        "        self.word2index = {'PAD': PAD_token}\n",
        "        self.index2word = {PAD_token: 'PAD'}\n",
        "        self.num_words = 1\n",
        "\n",
        "        count = 0\n",
        "        with open(self.vocab_file, 'r') as fid:\n",
        "            for line in fid:\n",
        "                if count != 0:\n",
        "                    line = line.strip().split(' ')\n",
        "                    word = line[0]\n",
        "                    if word not in self.word2index:\n",
        "                        self.word2index[word] = self.num_words\n",
        "                        self.index2word[self.num_words] = word\n",
        "                        self.num_words += 1\n",
        "                count += 1\n",
        "        UNK_token = self.num_words\n",
        "        BOS_token = self.num_words + 1\n",
        "        EOS_token = self.num_words + 2\n",
        "        BLANK_token = self.num_words + 3\n",
        "        self.word2index['<UNK>'] = UNK_token\n",
        "        self.word2index['<BOS>'] = BOS_token\n",
        "        self.word2index['<EOS>'] = EOS_token\n",
        "        self.word2index['<BLANK>'] = BLANK_token\n",
        "        self.index2word[UNK_token] = '<UNK>'\n",
        "        self.index2word[BOS_token] = '<BOS>'\n",
        "        self.index2word[EOS_token] = '<EOS>'\n",
        "        self.index2word[BLANK_token] = '<BLANK>'\n",
        "        self.num_words += 4\n",
        "\n",
        "class caffeFeatureLoader():\n",
        "    @staticmethod\n",
        "    def loadVideoC3DFeature(sample_name, feattype = 'pool5'):\n",
        "        featnames = glob.glob(os.path.join(sample_name, '*.' + feattype))\n",
        "        featnames.sort()\n",
        "        feat = []\n",
        "        for name in featnames:\n",
        "            feat.append(caffeFeatureLoader.loadC3DFeature(name)[0])\n",
        "        return feat\n",
        "\n",
        "    @staticmethod\n",
        "    def loadC3DFeature(filename):\n",
        "        feat = []\n",
        "        with open(filename, 'rb') as fileData:\n",
        "            num = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            chanel = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            length = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            height = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            width = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            blob_shape = [num, chanel, length, height, width]\n",
        "            m = num * chanel * length * height * width\n",
        "            for i in range(m):\n",
        "                val = struct.unpack(\"f\", fileData.read(4))[0]\n",
        "                feat.append(val)\n",
        "        return feat, blob_shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBBur-ZuX3QK"
      },
      "source": [
        "#Self-critic REINFORCE Implementation\n",
        "\n",
        "This class represents the agent used in the paper. The functions `select_action_sample` and `select_action_argmax` are used for training and inference/critic respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqnI8pbjBA5v"
      },
      "source": [
        "\n",
        "class SC_REINFORCE():\n",
        "    def __init__(self, opts):\n",
        "      self.opts = opts\n",
        "      print(self.opts)\n",
        "      self.model = Transformer(src_vocab_size = opts.src_vocab_size ,\n",
        "                                trg_vocab_size = opts.trg_vocab_size,\n",
        "                                src_pad_idx = opts.src_pad_idx,\n",
        "                                trg_pad_idx = opts.trg_pad_idx,\n",
        "                                embed_size=512,\n",
        "                               max_length = opts.max_len,\n",
        "                               device = opts.device,\n",
        "                               dropout=0.5,\n",
        "                               )\n",
        "      self.model = self.model.to(device)\n",
        "      self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "      self.model.train()\n",
        "      self.epsilon = 0.5\n",
        "      self.decay = 0.95\n",
        "      self.min_epsilon = 0.1\n",
        "\n",
        "    def select_action_sample(self, state):\n",
        "      #for R\n",
        "      # print(state['data'],state['gloss'])\n",
        "      probs = F.softmax(self.model(state['data'].to(device),state['gloss'][:,:].to(device)),-1)\n",
        "      # print(probs.shape, probs.min(),probs.max())       \n",
        "      # print(probs[:,1,:].min(), probs[:,1,:].max())\n",
        "      # if np.random.randn()>eps:\n",
        "      #   action = [ probs[:,i,:].multinomial(1).item() for i in range(np.shape(probs)[1]) ]\n",
        "      # else:\n",
        "      #   action = [ np.random.randint(0,np.shape(probs)[-1]) for i in range(np.shape(probs)[1])]\n",
        "      action = []\n",
        "      self.epsilon = max(self.min_epsilon, self.epsilon*self.decay)\n",
        "      for i in range(np.shape(probs)[1]):\n",
        "        if np.random.randn()>self.epsilon:\n",
        "          action.append(probs[:,i,:].multinomial(1).item())\n",
        "        else:\n",
        "          action.append(np.random.randint(0,np.shape(probs)[-1]))\n",
        "      # print('action: ',action)\n",
        "      prob = torch.stack([probs[:,i,a] for i,a in enumerate(action)])\n",
        "      log_prob = prob.log()\n",
        "      # print('log prob: ',log_prob.shape)\n",
        "      # entropy = - (probs*probs.log()).sum()\n",
        "      # print(\"action: \",action)\n",
        "      return action, log_prob\n",
        "\n",
        "    def select_action_argmax(self, state, inference = False):\n",
        "      # for R_hat\n",
        "      probs = F.softmax(self.model(state['data'].to(device),state['gloss'][:,:].to(device)),-1)\n",
        "      \n",
        "      # print(probs.shape, probs.min(),probs.max())       \n",
        "      # action = [probs[:,i,:].argmax().item() for i in range(np.shape(probs)[1]) ]\n",
        "      self.epsilon = max(self.min_epsilon, self.epsilon*self.decay)\n",
        "      action = []\n",
        "      for i in range(np.shape(probs)[1]):\n",
        "        if np.random.randn()>self.epsilon:\n",
        "          action.append(probs[:,i,:].argmax().item())\n",
        "        else:\n",
        "          action.append(np.random.randint(0,np.shape(probs)[-1]))\n",
        "      # print(action)\n",
        "      prob = probs[:, :,action].view(1, -1)\n",
        "      # log_prob = [prob[0].log()]\n",
        "      # print('log_prob: ',log_prob.shape)\n",
        "      # entropy = - (probs*probs.log()).sum()\n",
        "      return action\n",
        "    \n",
        "    def inference(self, ph_video, state):\n",
        "      #[dataset --> ph_video] \"ph_video.voc.word2index['<BOS>']\" = returns idx of the token\n",
        "\n",
        "      # inp  = ph_video.voc.word2index['<BOS>']\n",
        "      inp  = torch.tensor([ph_video.voc.word2index['<BOS>']]).unsqueeze(0).to(device)\n",
        "\n",
        "      while True:\n",
        "        # print(\"getting action\")\n",
        "        state['gloss'] = inp\n",
        "        action = self.select_action_argmax(state, inference=True)\n",
        "        # print(\"getting next word: \",action)\n",
        "        next_word = np.argmax(action,axis=-1)\n",
        "        # print(next_word)\n",
        "        # break\n",
        "      #   #next_word = probs;\n",
        "        # print(len(inp),np.shape(next_word))\n",
        "        inp = torch.cat([inp,torch.tensor([[next_word]])],dim=-1).to(device)                 \n",
        "        \n",
        "        # break\n",
        "        if next_word == ph_video.voc.word2index['<EOS>'] or np.shape(inp)[-1] >= self.opts.max_len:\n",
        "          break\n",
        "      return inp \n",
        "      \n",
        "    def update_parameters(self, rewards, rewards_argmax, log_probs, gamma = 1.):\n",
        "      # print(rewards, log_probs)\n",
        "      rewards, rewards_argmax = torch.tensor(rewards).float(), torch.tensor(rewards_argmax).float()\n",
        "      R = torch.zeros(1, 1)\n",
        "      loss = 0\n",
        "      R_hat = rewards_argmax.sum()\n",
        "      # print(np.shape(log_probs[0]),np.shape())\n",
        "      for i in reversed(range(len(rewards))):\n",
        "          R = gamma * R + rewards[i]\n",
        "          # print(\"update: \", log_probs[i].shape)\n",
        "          # l_p = torch.stack(log_probs[i])\n",
        "          loss = loss - (torch.matmul(log_probs[i].T,(Variable(R).expand_as(log_probs[i]).to(device) - Variable(R_hat).expand_as(log_probs[i]).to(device))))\n",
        "          # loss = loss - (torch.matmul(log_probs[i].T,(Variable(R).expand_as(log_probs[i]).to(device))))\n",
        "      loss = 0.5*loss / len(rewards)\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      utils.clip_grad_norm(self.model.parameters(), 80)\n",
        "      self.optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07thO0zPr6aB"
      },
      "source": [
        "# Enviornment class\n",
        "\n",
        "This class is designed to handle the dynamics of the data, i.e., moving from one state to another. The envrionment also lets us know when a particular video is completed and calculates the rewards for the actions taken in an episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcNkQZLZ7Gcw"
      },
      "source": [
        "class Environment():\n",
        "    def __init__(self, ph_dataset ,dataLoader, window_size = 8, skip = 4, phase = 'train'):\n",
        "      self.ph_dataset = ph_dataset\n",
        "      self.iterator = iter(dataLoader)\n",
        "      self.window_size = window_size\n",
        "      self.skip = skip\n",
        "      self.current_video = None\n",
        "      self.sample = None\n",
        "      self.gen = None\n",
        "      self.actions = []\n",
        "      self.num_vids = len(self.iterator)\n",
        "      self.phase = phase\n",
        "    \n",
        "    def append_a(self, actions, a):\n",
        "      if len(actions) == 0:\n",
        "        actions = a\n",
        "      else:\n",
        "        np.append(actions, a[int(len(a)/2):])\n",
        "      actions = np.array(actions).flatten()\n",
        "      return actions     \n",
        "\n",
        "\n",
        "    def next_video(self):\n",
        "      self.actions = []\n",
        "      self.sample = next(self.iterator)\n",
        "      self.gen = iter(self.gen_data_and_gloss())\n",
        "      data,_,_ = self.step()   \n",
        "      return data  \n",
        "\n",
        "    def gen_data_and_gloss(self):\n",
        "      sample = self.sample\n",
        "      for i in range(sample['data'].shape[1]):\n",
        "        if self.phase == 'train':\n",
        "          yield {'data': sample['data'][:,i,:], 'gloss': sample['gloss'][:,i,:], 'gloss_text': sample['gloss_text'][i]}\n",
        "        else:\n",
        "          yield{'data': sample['data'][:,i,:]}\n",
        "\n",
        "\n",
        "    def get_wer(self, ref,actions):\n",
        "      un_tags = np.where(np.logical_or(actions == self.ph_dataset.voc.word2index['<EOS>'], actions == self.ph_dataset.voc.word2index['<BOS>']))\n",
        "      actions = np.delete(actions, un_tags[0])\n",
        "      hyp_list = [i[0] for i in groupby(actions)]\n",
        "      ref = ' '.join(e for e in self.ph_dataset.index2sentence(ref.numpy()[0]))\n",
        "      hyp = ' '.join(e for e in self.ph_dataset.index2sentence(hyp_list))\n",
        "      self.ref = ref\n",
        "      self.hyp = hyp\n",
        "      wer = get_wer_delsubins(ref, hyp)\n",
        "      return wer[0]*100\n",
        "\n",
        "    def step(self,action = None):\n",
        "      done = False\n",
        "      reward = 0\n",
        "      if action != None:\n",
        "        self.actions = self.append_a(self.actions, action)\n",
        "      clip = next(self.gen, None)\n",
        "     \n",
        "      if clip == None:\n",
        "        done = True\n",
        "        self.ref = self.sample['label']\n",
        "        reward = 100 - self.get_wer(self.ref, self.actions)\n",
        "        \n",
        "      return clip, reward, done\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsuyPDW9r4Hy"
      },
      "source": [
        "# WER calculation\n",
        "\n",
        "Word error rate is given by:\n",
        "\n",
        "$WER = \\frac{S + I + D}{N} \\times 100$\n",
        "\n",
        "where $S, I, D$ refer to the number of substitution, insertion and deletion a sentence needs to look like a reference sentence. $N$ is the total number of words in the hypothesis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k43uXYVzBTLH"
      },
      "source": [
        "def get_wer_delsubins(ref, hyp, debug=False):\n",
        "    DEL_PENALTY = 1\n",
        "    SUB_PENALTY = 1\n",
        "    INS_PENALTY = 1\n",
        "    r = ref\n",
        "    h = hyp\n",
        "    # costs will holds the costs, like in the Levenshtein distance algorithm\n",
        "    costs = [[0 for inner in range(len(h) + 1)] for outer in range(len(r) + 1)]\n",
        "    # backtrace will hold the operations we've done.\n",
        "    # so we could later backtrace, like the WER algorithm requires us to.\n",
        "    backtrace = [[0 for inner in range(len(h) + 1)] for outer in range(len(r) + 1)]\n",
        "\n",
        "    OP_OK = 0\n",
        "    OP_SUB = 1\n",
        "    OP_INS = 2\n",
        "    OP_DEL = 3\n",
        "\n",
        "    # First column represents the case where we achieve zero\n",
        "    # hypothesis words by deleting all reference words.\n",
        "    for i in range(1, len(r) + 1):\n",
        "        costs[i][0] = DEL_PENALTY * i\n",
        "        backtrace[i][0] = OP_DEL\n",
        "\n",
        "    # First row represents the case where we achieve the hypothesis\n",
        "    # by inserting all hypothesis words into a zero-length reference.\n",
        "    for j in range(1, len(h) + 1):\n",
        "        costs[0][j] = INS_PENALTY * j\n",
        "        backtrace[0][j] = OP_INS\n",
        "\n",
        "    # computation\n",
        "    for i in range(1, len(r) + 1):\n",
        "        for j in range(1, len(h) + 1):\n",
        "            if r[i - 1] == h[j - 1]:\n",
        "                costs[i][j] = costs[i - 1][j - 1]\n",
        "                backtrace[i][j] = OP_OK\n",
        "            else:\n",
        "                substitutionCost = costs[i - 1][j - 1] + SUB_PENALTY  # penalty is always 1\n",
        "                insertionCost = costs[i][j - 1] + INS_PENALTY  # penalty is always 1\n",
        "                deletionCost = costs[i - 1][j] + DEL_PENALTY  # penalty is always 1\n",
        "\n",
        "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
        "                if costs[i][j] == substitutionCost:\n",
        "                    backtrace[i][j] = OP_SUB\n",
        "                elif costs[i][j] == insertionCost:\n",
        "                    backtrace[i][j] = OP_INS\n",
        "                else:\n",
        "                    backtrace[i][j] = OP_DEL\n",
        "\n",
        "    # back trace though the best route:\n",
        "    i = len(r)\n",
        "    j = len(h)\n",
        "    numSub = 0\n",
        "    numDel = 0\n",
        "    numIns = 0\n",
        "    numCor = 0\n",
        "    if debug:\n",
        "        print(\"OP\\tREF\\tHYP\")\n",
        "        lines = []\n",
        "    while i > 0 or j > 0:\n",
        "        if backtrace[i][j] == OP_OK:\n",
        "            numCor += 1\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "            if debug:\n",
        "                lines.append(\"OK\\t\" + r[i] + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_SUB:\n",
        "            numSub += 1\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "            if debug:\n",
        "                lines.append(\"SUB\\t\" + r[i] + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_INS:\n",
        "            numIns += 1\n",
        "            j -= 1\n",
        "            if debug:\n",
        "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_DEL:\n",
        "            numDel += 1\n",
        "            i -= 1\n",
        "            if debug:\n",
        "                lines.append(\"DEL\\t\" + r[i] + \"\\t\" + \"****\")\n",
        "    if debug:\n",
        "        lines = reversed(lines)\n",
        "        for line in lines:\n",
        "            print(line)\n",
        "        print(\"#cor \" + str(numCor))\n",
        "        print(\"#sub \" + str(numSub))\n",
        "        print(\"#del \" + str(numDel))\n",
        "        print(\"#ins \" + str(numIns))\n",
        "    return (numSub + numDel + numIns) / (float)(len(r)), numSub / float(len(r)), numIns / float(len(r)), numDel / float(len(r))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jZqshlkmXJR"
      },
      "source": [
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/RL_Project_CodeBase\"\n",
        "PHOE_DIR = os.path.join(BASE_DIR,'phoenix2014-release/phoenix-2014-multisigner')\n",
        "FEAT_DIR = os.path.join(BASE_DIR,'c3d_res_phoenix_body_iter5_120k')\n",
        "ANNOT_DIR = os.path.join(PHOE_DIR,'annotations')\n",
        "CORPUS_DIR = os.path.join(ANNOT_DIR,'manual')\n",
        "\n",
        "\n",
        "# env = ENV()\n",
        "# helper function append_action(actions, a): returns appended action after ignoring 4 overlap frames\n",
        "train_dataset = PhoenixVideo(CORPUS_DIR, ANNOT_DIR, FEAT_DIR, 'train' )\n",
        "train_loader = DataLoader(train_dataset,\n",
        "    batch_size=1, shuffle=True)\n",
        "\n",
        "dev_dataset = PhoenixVideo(CORPUS_DIR, ANNOT_DIR, FEAT_DIR, 'dev' )\n",
        "dev_loader = DataLoader(dev_dataset,\n",
        "    batch_size=1, shuffle=True)\n",
        "\n",
        "test_dataset = PhoenixVideo(CORPUS_DIR, ANNOT_DIR, FEAT_DIR, 'test' )\n",
        "test_loader = DataLoader(test_dataset,\n",
        "    batch_size=1, shuffle=True)\n",
        "\n",
        "env = Environment(train_dataset, train_loader)\n",
        "env2 = Environment(train_dataset, train_loader)\n",
        "\n",
        "env_dev = Environment(dev_dataset, dev_loader, phase='dev')\n",
        "env_test = Environment(test_dataset, test_loader, phase='test')\n",
        "\n",
        "opts = ED({\n",
        "    \"src_vocab_size\": 512,\n",
        "    \"trg_vocab_size\": train_dataset.voc.num_words,\n",
        "    \"src_pad_idx\": 0,\n",
        "    \"trg_pad_idx\": 0,\n",
        "    \"embed_size\" : 512,\n",
        "    \"num_layers\": 6,\n",
        "    \"forward_expansion\": 4,\n",
        "    \"heads\": 8,\n",
        "    \"dropout\": 0.3,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
        "    \"max_len\": 520,\n",
        "    \"log_freq\":10,\n",
        "})\n",
        "\n",
        "agent = SC_REINFORCE(opts) # add args \n",
        "train_wers = []\n",
        "dev_wers = []\n",
        "test_wers = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3a3Qir7HMes"
      },
      "source": [
        "#debug\n",
        "# state = env.next_video()\n",
        "# for key, values in state.items():\n",
        "#   print(\"{}: \".format(key),np.shape(values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1VxMKTUKMFP"
      },
      "source": [
        "#debug\n",
        "# print(state['gloss'])\n",
        "# print(state['gloss_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCFSVgRNsrTS"
      },
      "source": [
        "# Main Training and Inference Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93TJNDwMPI5L"
      },
      "source": [
        "\n",
        "\n",
        "for vid_num in range(env.num_vids): #env.num_epochs\n",
        "  # print('*'*5, \"NEW VIDEO\", '*'*5)\n",
        "  state = env.next_video()\n",
        "  state2 = env2.next_video()\n",
        "  # print('data: ', np.shape(env.sample['data']), 'label: ', np.shape(env.sample['gloss']))\n",
        " \n",
        "  reward_s = []\n",
        "  reward_a = []\n",
        "  log_probs = []\n",
        "  done_s = done_a = False\n",
        "\n",
        "  while not done_s or not done_a:\n",
        "    \n",
        "    if not done_s:\n",
        "      a_s, log_prob = agent.select_action_sample(state)\n",
        "      state, r_s, done_s = env.step(a_s)\n",
        "      reward_s.append(r_s)\n",
        "      log_probs.append(log_prob)\n",
        "   \n",
        "    if not done_a:\n",
        "      a_a = agent.select_action_argmax(state2)\n",
        "      state2, r_a, done_a = env2.step(a_a)\n",
        "      reward_a.append(r_a)\n",
        "\n",
        "    if done_a and done_s:\n",
        "      break\n",
        "  if vid_num % opts.log_freq == 0:\n",
        "    print(\"*\"*100)\n",
        "    print(\"video: \",vid_num)\n",
        "    print(\"ref: \",env.ref)\n",
        "    print(\"hyp_s: \",env.hyp)\n",
        "    print(\"hyp_a: \",env2.hyp)\n",
        "    print(\"wer:\", np.mean(train_wers))\n",
        "    if not os.path.exists(os.path.join(BASE_DIR,'Logs/')):\n",
        "      os.makedirs(os.path.join(BASE_DIR,'Logs/'))\n",
        "    torch.save(agent.model.state_dict(), os.path.join(BASE_DIR,'Logs/trial.pt'))\n",
        "  train_wers.append(1 - reward_s[-1])    \n",
        "  reward_a, reward_s, log_probs = np.array(reward_a), np.array(reward_s), np.array(log_probs)\n",
        "  # print(\"action: \",env.actions)\n",
        "  # print(\"reward and prob: \",np.shape(reward_a),np.shape(log_probs))\n",
        "  agent.update_parameters(reward_s, reward_a,log_probs=log_probs)\n",
        "print(\"Train WER AVG: \", np.mean(train_wers))\n",
        "\n",
        "# agent.inference(state)\n",
        "for epoch in range(env_dev.num_vids):\n",
        "  print('*'*5, \"NEW VIDEO\", '*'*5)\n",
        "  state = env_dev.next_video()\n",
        "  print('data: ', np.shape(env_dev.sample['data']), 'label: ', np.shape(env_dev.sample['label']))\n",
        " \n",
        "  rewards = []\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    \n",
        "    a = agent.inference(dev_dataset,state)\n",
        "    state, r, done = env_dev.step(a)\n",
        "    rewards.append(r)\n",
        "  dev_wers.append(1 - rewards[-1])\n",
        "\n",
        "print(\"Dev WER AVG: \", np.mean(dev_wers))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdP1KaCys7ZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA7z1g42s7bK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M_8kKY2s7da"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZNZhCSSs8E6"
      },
      "source": [
        "# Supervised Learning route\n",
        "\n",
        "To cross-test the issues, the SL method of the model was also studied.\n",
        "\n",
        "`SmoothCrossEntropyLoss` is used for applying label smoothing to the output and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZr1UTDnOjA7"
      },
      "source": [
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "\n",
        "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def k_one_hot(self, targets:torch.Tensor, n_classes:int, smoothing=0.0):\n",
        "        with torch.no_grad():\n",
        "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
        "                                  device=targets.device) \\\n",
        "                                  .fill_(smoothing /(n_classes-1)) \\\n",
        "                                  .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n",
        "        return targets\n",
        "\n",
        "    def reduce_loss(self, loss):\n",
        "        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n",
        "        if self.reduction == 'sum' else loss\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        assert 0 <= self.smoothing < 1\n",
        "\n",
        "        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n",
        "        log_preds = F.log_softmax(inputs, -1)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            log_preds = log_preds * self.weight.unsqueeze(0)\n",
        "\n",
        "        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzFxrQX7tMYF"
      },
      "source": [
        " # Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOFNcC_YcGYR"
      },
      "source": [
        "\n",
        "def collate_fn_video(batch, padding=6):\n",
        "    # batch.sort(key=lambda x: x['data'].shape[0], reverse=True)\n",
        "    len_video = [x['data'].shape[0] for x in batch]\n",
        "    len_label = [len(x['label']) for x in batch]\n",
        "    batch_video = torch.zeros(len(len_video), max(len_video), batch[0]['data'].shape[1])\n",
        "    batch_label = []\n",
        "    IDs = []\n",
        "    for i, bat in enumerate(batch):\n",
        "        data = bat['data']\n",
        "        label = bat['label']\n",
        "        batch_label.extend(label)\n",
        "        batch_video[i, :len_video[i], :] = torch.FloatTensor(data)\n",
        "        IDs.append(bat['id'])\n",
        "    batch_label = torch.LongTensor(batch_label)\n",
        "    len_video = torch.LongTensor(len_video)\n",
        "    len_label = torch.LongTensor(len_label)\n",
        "\n",
        "    batch_video = batch_video.permute(0, 2, 1)\n",
        "    return {'data': batch_video, 'label': batch_label, 'len_data': len_video, 'len_label': len_label, 'id': IDs}\n",
        "\n",
        "def collate_fn_clip(batch):\n",
        "    IDs = []\n",
        "    video_list = []\n",
        "    label_list = {}\n",
        "    gloss_list = []\n",
        "    for i, bat in enumerate(batch):\n",
        "        data = bat['data']\n",
        "        label = bat['label']\n",
        "        IDs.append(bat['id'])\n",
        "        gloss = bat['gloss']\n",
        "        # print(data.shape, gloss.shape)\n",
        "        gloss_list.append(torch.tensor(gloss))\n",
        "        video_list.append(data)\n",
        "        label_list[bat['id']] = torch.tensor(label).unsqueeze(0)\n",
        "    # label_tensor = torch.LongTensor(label_list)\n",
        "    gloss_tensor = torch.cat(gloss_list, dim=0)\n",
        "    video_tensor = torch.cat(video_list, dim=0)\n",
        "    return  {'data': video_tensor, 'label': label_list, 'id': IDs, 'gloss': gloss_tensor}\n",
        "\n",
        "\n",
        "smoothLoss = SmoothCrossEntropyLoss(smoothing = 0.2)\n",
        "def SL_train(model ,clip, label):\n",
        "  model.train()  \n",
        "  clip, label = clip.to(device), label.to(device)\n",
        "  preds = model(clip, label[:,:-1])\n",
        "  # print(preds.max(-1))\n",
        "  # preds = preds.argmax(dim=-1)              \n",
        "  optimi.zero_grad()\n",
        "  # print(preds.shape,label[:,1:].shape)\n",
        "  # loss = F.cross_entropy(preds.view(-1, preds.size(-1)).float(), label[0,1:].view(-1)) #dont know if we need this \n",
        "  # print(preds.view(-1, preds.size(-1)).shape, label[:,1:].reshape(-1).shape)\n",
        "  loss = smoothLoss(preds.view(-1, preds.size(-1)).float(), label[:,1:].reshape(-1))\n",
        "  loss.backward()\n",
        "  optimi.step()\n",
        "\n",
        "  return preds.argmax(dim=-1).squeeze().detach().cpu()\n",
        "\n",
        "train_dataset = PhoenixVideo(CORPUS_DIR, ANNOT_DIR, FEAT_DIR, 'train' )\n",
        "train_loader = DataLoader(train_dataset,\n",
        "    batch_size=1, shuffle=False, collate_fn=collate_fn_clip)\n",
        "\n",
        "\n",
        "\n",
        "agent = SC_REINFORCE(opts) # add args \n",
        "\n",
        "env = Environment(train_dataset, train_loader)\n",
        "optimi = optim.Adam(agent.model.parameters(),lr = 1e-6)\n",
        "\n",
        "wers = []\n",
        "for i,batch in enumerate(train_loader):\n",
        "  data, gloss = torch.tensor(batch['data']).squeeze(), torch.tensor(batch['gloss'])\n",
        "  print(data.shape, gloss.shape)\n",
        "  a = SL_train(agent.model, data, gloss)\n",
        "  actions = []\n",
        "  actions = env.append_a(actions,a)\n",
        "  # print(np.shape(actions[0:10]))\n",
        "  n=0\n",
        "  for keys,values in batch['label'].items():\n",
        "    offset = batch['data'].shape[0]*512\n",
        "    aa = np.array([actions[n:n+offset]])\n",
        "    n+=offset\n",
        "    # print(values.shape,aa[0])\n",
        "    wers.append(1-env.get_wer(values,aa))\n",
        "  if (i+1) % opts.log_freq==0:\n",
        "    print(\"*\"*100)\n",
        "    print(\"video: \",i+1)\n",
        "    print(\"ref: \",env.ref)\n",
        "    print(\"hyp_s: \",env.hyp)\n",
        "    # print(\"hyp_a: \",env2.hyp)\n",
        "    print(\"wer:\", np.mean(wers))\n",
        "    if not os.path.exists(os.path.join(BASE_DIR,'Logs/')):\n",
        "      os.makedirs(os.path.join(BASE_DIR,'Logs/'))\n",
        "    torch.save(agent.model.state_dict(), os.path.join(BASE_DIR,'Logs/trial_SL_Epoch{}.pt'.format(i+1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}