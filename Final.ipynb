{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PandaBoi/CSLR_with_RL/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxLGZCGgioEj"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v04elr6viozQ",
        "outputId": "c0a8bdea-121d-4ffe-e305-5a05450b95cc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjpffKpc1vSn",
        "outputId": "a3d5d227-7a8b-46ca-c893-b90f2414851b"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "import torchvision.transforms as T\n",
        "from torch.autograd import Variable\n",
        "import pdb\n",
        "from itertools import groupby\n",
        "!pip install easydict\n",
        "from easydict import EasyDict as ED\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#maybe will need for later\n",
        "# !pip install git+https://github.com/enhuiz/phoenix-datasets\n",
        "# !pip install xmltodict\n",
        "# from phoenix_datasets import PhoenixVideoTextDataset\n",
        " \n",
        "# from torch.utils.data import DataLoader\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNi1eEMiAztu"
      },
      "source": [
        "def SL_train(clip, label):\n",
        "  model.train()  \n",
        "  preds = model(clip.to(device), label[:,:-1].to(device))              \n",
        "  optim.zero_grad()\n",
        "  \n",
        "  preds = F.cross_entropy(preds.view(-1, preds.size(-1)), label[:,1:].view(-1,label[:,1:]),\n",
        "                         ignore_index=target_pad) #dont know if we need this \n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  \n",
        "  total_loss += loss.data[0]\n",
        "  if (i + 1) % print_every == 0:\n",
        "      loss_avg = total_loss / print_every\n",
        "      print(\"time = %dm, epoch %d, iter = %d, loss = %.3f,\\\n",
        "      %ds per %d iters\" % ((time.time() - start) // 60,\\\n",
        "      epoch + 1, i + 1, loss_avg, time.time() - temp,\\\n",
        "      print_every))\n",
        "      total_loss = 0\n",
        "      temp = time.time()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGzEbRhxBANQ"
      },
      "source": [
        "\n",
        "def get_inplanes():\n",
        "    return [64, 128, 256, 512]\n",
        "\n",
        "\n",
        "def conv3x3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv3d(in_planes,\n",
        "                     out_planes,\n",
        "                     kernel_size=3,\n",
        "                     stride=stride,\n",
        "                     padding=1,\n",
        "                     bias=False)\n",
        "\n",
        "\n",
        "def conv1x1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv3d(in_planes,\n",
        "                     out_planes,\n",
        "                     kernel_size=1,\n",
        "                     stride=stride,\n",
        "                     bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv1x1x1(in_planes, planes)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 block,\n",
        "                 layers,\n",
        "                 block_inplanes,\n",
        "                 n_input_channels=3,\n",
        "                 conv1_t_size=7,\n",
        "                 conv1_t_stride=1,\n",
        "                 no_max_pool=False,\n",
        "                 shortcut_type='B',\n",
        "                 widen_factor=1.0,\n",
        "                 n_classes=400):\n",
        "        super().__init__()\n",
        "\n",
        "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
        "\n",
        "        self.in_planes = block_inplanes[0]\n",
        "        self.no_max_pool = no_max_pool\n",
        "\n",
        "        self.conv1 = nn.Conv3d(n_input_channels,\n",
        "                               self.in_planes,\n",
        "                               kernel_size=(conv1_t_size, 7, 7),\n",
        "                               stride=(conv1_t_stride, 2, 2),\n",
        "                               padding=(conv1_t_size // 2, 3, 3),\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
        "                                       shortcut_type)\n",
        "        self.layer2 = self._make_layer(block,\n",
        "                                       block_inplanes[1],\n",
        "                                       layers[1],\n",
        "                                       shortcut_type,\n",
        "                                       stride=2)\n",
        "        self.layer3 = self._make_layer(block,\n",
        "                                       block_inplanes[2],\n",
        "                                       layers[2],\n",
        "                                       shortcut_type,\n",
        "                                       stride=2)\n",
        "        self.layer4 = self._make_layer(block,\n",
        "                                       block_inplanes[3],\n",
        "                                       layers[3],\n",
        "                                       shortcut_type,\n",
        "                                       stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight,\n",
        "                                        mode='fan_out',\n",
        "                                        nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _downsample_basic_block(self, x, planes, stride):\n",
        "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
        "                                out.size(3), out.size(4))\n",
        "        if isinstance(out.data, torch.FloatTensor.to(device)):\n",
        "            zero_pads = zero_pads.to(device)\n",
        "\n",
        "        out = torch.cat([out.data, zero_pads], dim=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(self._downsample_basic_block,\n",
        "                                     planes=planes * block.expansion,\n",
        "                                     stride=stride)\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
        "                    nn.BatchNorm3d(planes * block.expansion))\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(in_planes=self.in_planes,\n",
        "                  planes=planes,\n",
        "                  stride=stride,\n",
        "                  downsample=downsample))\n",
        "        self.in_planes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.in_planes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        if not self.no_max_pool:\n",
        "            x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def generate_model(model_depth, **kwargs):\n",
        "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
        "\n",
        "    if model_depth == 10:\n",
        "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 18:\n",
        "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 34:\n",
        "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 50:\n",
        "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 101:\n",
        "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 152:\n",
        "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
        "    elif model_depth == 200:\n",
        "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd5UHz3LA6Fu"
      },
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, heads, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "    ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size,)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        out = self.dropout(\n",
        "              x.float().unsqueeze(0), \n",
        "              # self.word_embedding(x) \n",
        "            # + self.position_embedding(positions)\n",
        "        )\n",
        "\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\n",
        "        # decoder this will change. This might look a bit odd in this case.\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "        # x = self.dropout(x+ self.position(position_embedding(positions)))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0.3,\n",
        "        device=\"cpu\",\n",
        "        max_length=100,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        out = F.softmax(out,-1)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqnI8pbjBA5v"
      },
      "source": [
        "\n",
        "class SC_REINFORCE():\n",
        "    def __init__(self, opts):\n",
        "      self.opts = opts\n",
        "      print(self.opts)\n",
        "      self.model = Transformer(src_vocab_size = opts.src_vocab_size ,\n",
        "                                trg_vocab_size = opts.trg_vocab_size,\n",
        "                                src_pad_idx = opts.src_pad_idx,\n",
        "                                trg_pad_idx = opts.trg_pad_idx,\n",
        "                                embed_size=512,\n",
        "                               max_length = opts.max_len,\n",
        "                               )\n",
        "      self.model = self.model.to(device)\n",
        "      self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "      self.model.train()\n",
        "\n",
        "    def select_action_sample(self, state):\n",
        "      #for R\n",
        "      # print(state['data'],state['gloss'])\n",
        "      probs = self.model(state['data'],state['gloss'][:,:-1])\n",
        "      # print(probs.shape, probs.min(),probs.max())       \n",
        "      action = [probs[:,i,:].multinomial(1).item() for i in range(np.shape(probs)[1]) ]\n",
        "      # print(action)\n",
        "      prob = probs[:, :,action].view(1, -1)\n",
        "      prob = [probs[:,i,a] for i,a in enumerate(action)]\n",
        "      log_prob = prob[0].log().unsqueeze(0)\n",
        "      # print('log prob: ',log_prob.shape)\n",
        "      # entropy = - (probs*probs.log()).sum()\n",
        "      # print(\"action: \",action)\n",
        "      return action, log_prob\n",
        "\n",
        "    def select_action_argmax(self, state):\n",
        "      # for R_hat\n",
        "      probs = self.model(state['data'],state['gloss'][:,:-1])\n",
        "      # print(probs.shape, probs.min(),probs.max())       \n",
        "      action = [probs[:,i,:].argmax().item() for i in range(np.shape(probs)[1]) ]\n",
        "      # print(action)\n",
        "      prob = probs[:, :,action].view(1, -1)\n",
        "      # log_prob = [prob[0].log()]\n",
        "      # print('log_prob: ',log_prob.shape)\n",
        "      # entropy = - (probs*probs.log()).sum()\n",
        "      return action\n",
        "    \n",
        "    def inference(self,state):\n",
        "      #[dataset --> ph_video] \"ph_video.voc.word2index['<BOS>']\" = returns idx of the token\n",
        "\n",
        "      # inp  = ph_video.voc.word2index['<BOS>']\n",
        "      inp  = torch.tensor([ph_video.voc.word2index['<BOS>']]).unsqueeze(0).to(device)\n",
        "\n",
        "      while True:\n",
        "        # print(\"getting action\")\n",
        "        action = self.select_action_argmax(state)\n",
        "        # print(\"getting next word\")\n",
        "        next_word = np.argmax(action,axis=-1)\n",
        "        # print(next_word)\n",
        "        # break\n",
        "      #   #next_word = probs;\n",
        "        # print(len(inp),np.shape(next_word))\n",
        "        inp = torch.cat([inp,torch.tensor([[next_word]])],dim=-1)                 \n",
        "        \n",
        "        # break\n",
        "        if next_word == ph_video.voc.word2index['<EOS>'] or np.shape(inp)[-1] >= self.opts.max_len:\n",
        "          break\n",
        "      return inp \n",
        "      \n",
        "    def update_parameters(self, rewards, rewards_argmax, log_probs, gamma = 1.):\n",
        "      rewards, rewards_argmax = torch.tensor(rewards), torch.tensor(rewards_argmax)\n",
        "      R = torch.zeros(1, 1)\n",
        "      loss = 0\n",
        "      R_hat = rewards_argmax.sum()\n",
        "      # print(np.shape(log_probs[0]),np.shape())\n",
        "      for i in reversed(range(len(rewards))):\n",
        "          R = gamma * R + rewards[i]\n",
        "          loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i]).to(device) - Variable(R_hat).expand_as(log_probs[i]).to(device)))\n",
        "      loss = loss / len(rewards)\n",
        "      # print(loss)\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      utils.clip_grad_norm(self.model.parameters(), 40)\n",
        "      self.optimizer.step()\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n62JQwuv58lo"
      },
      "source": [
        "import os\n",
        "import struct\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "\n",
        "class PhoenixVideo(Dataset):\n",
        "    def __init__(self, corpus_dir, ANNOT_DIR, video_path, phase, DEBUG=False):\n",
        "        self.ANNOT_DIR = ANNOT_DIR\n",
        "        self.vocab_file = os.path.join(ANNOT_DIR,'automatic/newtrainingClasses.txt')\n",
        "        self.image_type = 'png'\n",
        "        self.max_video_len = 10000\n",
        "        self.corpus_dir = corpus_dir\n",
        "        self.video_path = video_path\n",
        "        self.phase = phase\n",
        "        self.alignment = {}\n",
        "        self.voc = Voc(self.vocab_file)\n",
        "\n",
        "        self.phoenix_dataset = self.load_video_list()\n",
        "        # print(self.phoenix_dataset)\n",
        "        self.data_dict = self.phoenix_dataset[phase]\n",
        "        if DEBUG == True:\n",
        "            self.data_dict = self.data_dict[:101]\n",
        "        logging.info('[DATASET: {:s}]: total {:d} samples.'.format(phase, len(self.data_dict)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_dict)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cur_vid_info = self.data_dict[idx]\n",
        "        id = cur_vid_info['id']\n",
        "        video_file = cur_vid_info['path']\n",
        "        label = cur_vid_info['label']\n",
        "        label_text = cur_vid_info['label_text']\n",
        "        video_tensor = self.load_video(video_file)\n",
        "        sample = {'id': id, 'data': video_tensor, 'label': label, 'label_text':label_text}\n",
        "        if self.phase == 'train':\n",
        "          glosses = cur_vid_info['gloss'].split(' ')\n",
        "          # print(\"The shape of glosses is\", np.shape(glosses))\n",
        "          # print(\"The shape of data is\", np.shape(video_tensor))\n",
        "          # print(\"The shape of label is\", len(label))\n",
        "          # print(\"The shape of label_text is\", len(label_text))\n",
        "          sample['gloss'], sample['gloss_text'] = self.clip_glosses(sample, glosses)\n",
        "        return sample # final return (data: (num_clips,512), label:(1,len_of_sentence), gloss: (num_clips,10), id:(1))\n",
        "\n",
        "    def clip_glosses(self, sample, glosses, window_size = 8, skip_size = 4):\n",
        "      #input shape: (1,num_of_frames_in_video)\n",
        "      # segment gloss sentence into clip sizes (num_clips,8) [assert len(data) == num_clips]\n",
        "      # add BOS and EOS to each glossclip (num_clips, 10)\n",
        "      gloss_text_clips = []\n",
        "      gloss_clips = []\n",
        "      j = 0\n",
        "      for i in range(0, len(glosses) - window_size, skip_size):\n",
        "        j = j + 1\n",
        "        gloss_clip = []\n",
        "        gloss_clip.append('<BOS>')\n",
        "        gloss_clip.extend(glosses[i:i+window_size])\n",
        "        gloss_clip.append('<EOS>')\n",
        "        gloss_clip = [] + gloss_clip\n",
        "        gloss_clips.append(self.sentence2index(gloss_clip))\n",
        "        gloss_text_clips.append(gloss_clip)\n",
        "      gloss_clips = np.array(gloss_clips)\n",
        "      # gloss_text_clips = np.array(gloss_text_clips)\n",
        "      print(\"Len of gloss clips is\", np.shape(gloss_clips))\n",
        "      print(\"Shape of data is\", sample['data'].shape)\n",
        "      assert sample['data'].shape[0] == np.shape(gloss_clips)[0]\n",
        "      return gloss_clips, gloss_text_clips\n",
        "\n",
        "    \n",
        "    def load_video(self, video_name):\n",
        "        print('video name: ',video_name)\n",
        "        feat = caffeFeatureLoader.loadVideoC3DFeature(video_name, 'pool5')\n",
        "        feat = torch.tensor(feat)\n",
        "        return feat\n",
        "\n",
        "    def load_video1(self, video_name):\n",
        "        frames_list = glob.glob(os.path.join(video_name, '*.{:s}'.format(self.image_type)))\n",
        "        frames_list.sort()\n",
        "        num_frame = len(frames_list)\n",
        "        if self.phase=='train' and self.sample and num_frame > self.max_video_len:\n",
        "            for _ in range(num_frame-self.max_video_len):\n",
        "                frames_list.pop(np.random.randint(len(frames_list)))\n",
        "        frames_tensor_list = [self.load_image(frame_file) for frame_file in frames_list]\n",
        "        video_tensor = torch.stack(frames_tensor_list, dim=0)\n",
        "        return video_tensor\n",
        "\n",
        "    def load_image(self, img_name):\n",
        "        image = Image.open(img_name)\n",
        "        image = self.transform(image)\n",
        "        return image\n",
        "    \n",
        "    def load_gloss(self): # TO DO using:- https://github.com/enhuiz/phoenix-datasets/blob/570481bf03a46555ca219f79ace1a2cfab149f8c/phoenix_datasets/corpora.py#L35\n",
        "        gloss_path = os.path.join(self.ANNOT_DIR,'automatic/train.alignment')\n",
        "        read = partial(pd.read_csv, sep=\" \", na_filter=False)\n",
        "        ali = read(gloss_path, header=None, names=[\"id\", \"classlabel\"])\n",
        "        cls = read(os.path.join(self.ANNOT_DIR,\"automatic/trainingClasses.txt\"))\n",
        "        # print(cls)\n",
        "        df = pd.merge(ali, cls, how=\"left\", on=\"classlabel\")\n",
        "        # print(df)\n",
        "        del df[\"classlabel\"]\n",
        "        # print(df[\"signstate\"])\n",
        "        df[\"gloss\"] = df[\"signstate\"].apply(lambda s: s.rstrip(\"012\"))\n",
        "\n",
        "        df[\"id\"] = df[\"id\"].apply(lambda s: \"/\".join(s.split(\"/\")[3:-2]))\n",
        "        grouped = df.groupby(\"id\")\n",
        "\n",
        "        gdf = grouped[\"gloss\"].agg(\" \".join)\n",
        "        sdf = grouped[\"signstate\"].agg(\" \".join)\n",
        "\n",
        "        df = pd.merge(gdf, sdf, \"inner\", \"id\")\n",
        "        # print(df.loc['01April_2010_Thursday_heute_default-0'])\n",
        "        return df\n",
        "\n",
        "\n",
        "    def load_video_list(self):\n",
        "        phoenix_dataset = {}\n",
        "        outliers = ['13April_2011_Wednesday_tagesschau_default-14'] # '05July_2010_Monday_heute_default-8'\n",
        "        for task in ['train', 'dev', 'test']:\n",
        "            if task != self.phase:\n",
        "                continue\n",
        "            dataset_path = os.path.join(self.video_path, task)\n",
        "            corpus = pd.read_csv(os.path.join(self.corpus_dir, '{:s}.corpus.csv'.format(task)), sep='|')\n",
        "            videonames = corpus['folder'].values\n",
        "            annotation = corpus['annotation'].values\n",
        "            if self.phase == 'train':\n",
        "              glosses_df = self.load_gloss()\n",
        "              # print(glosses_df.columns)\n",
        "            ids = corpus['id'].values\n",
        "            num_sample = len(ids)\n",
        "            # print('num_sample: ',num_sample)\n",
        "            video_infos = []\n",
        "            for i in range(num_sample):\n",
        "                if ids[i] in outliers:\n",
        "                    continue\n",
        "                tmp_info = {\n",
        "                    'id': ids[i],\n",
        "                    'path': os.path.join(self.video_path, task, videonames[i].replace('*.png', '')),\n",
        "                    'label_text': annotation[i],\n",
        "                    'label': np.array(self.sentence2index(annotation[i].split(' ')))\n",
        "                }\n",
        "                if self.phase == 'train':\n",
        "                  try:\n",
        "                    tmp_info['gloss'] = glosses_df.loc[ids[i]]['gloss']\n",
        "                  except:\n",
        "                    # print('pass')\n",
        "                    continue\n",
        "                video_infos.append(tmp_info)\n",
        "            phoenix_dataset[task] = video_infos\n",
        "        return phoenix_dataset\n",
        "\n",
        "    def sentence2index(self, sent):\n",
        "        #sent = sent.split(' ')\n",
        "        # print(sent)\n",
        "        s = []\n",
        "        for word in sent:\n",
        "            if word in self.voc.word2index:\n",
        "                s.append(self.voc.word2index[word])\n",
        "            else:\n",
        "                s.append(self.voc.word2index['<UNK>'])\n",
        "        return s\n",
        "\n",
        "    def index2sentence(self, indices):\n",
        "      return [self.voc.index2word[ind] for ind in indices]\n",
        "\n",
        "\n",
        "class Voc():\n",
        "    def __init__(self, vocab_file):\n",
        "        PAD_token = 0\n",
        "        self.vocab_file = vocab_file\n",
        "        self.word2index = {'PAD': PAD_token}\n",
        "        self.index2word = {PAD_token: 'PAD'}\n",
        "        self.num_words = 1\n",
        "\n",
        "        count = 0\n",
        "        with open(self.vocab_file, 'r') as fid:\n",
        "            for line in fid:\n",
        "                if count != 0:\n",
        "                    line = line.strip().split(' ')\n",
        "                    word = line[0]\n",
        "                    if word not in self.word2index:\n",
        "                        self.word2index[word] = self.num_words\n",
        "                        self.index2word[self.num_words] = word\n",
        "                        self.num_words += 1\n",
        "                count += 1\n",
        "        UNK_token = self.num_words\n",
        "        BOS_token = self.num_words + 1\n",
        "        EOS_token = self.num_words + 2\n",
        "        BLANK_token = self.num_words + 3\n",
        "        self.word2index['<UNK>'] = UNK_token\n",
        "        self.word2index['<BOS>'] = BOS_token\n",
        "        self.word2index['<EOS>'] = EOS_token\n",
        "        self.word2index['<BLANK>'] = BLANK_token\n",
        "        self.index2word[UNK_token] = '<UNK>'\n",
        "        self.index2word[BOS_token] = '<BOS>'\n",
        "        self.index2word[EOS_token] = '<EOS>'\n",
        "        self.index2word[BLANK_token] = '<BLANK>'\n",
        "        self.num_words += 4\n",
        "\n",
        "class caffeFeatureLoader():\n",
        "    @staticmethod\n",
        "    def loadVideoC3DFeature(sample_name, feattype = 'pool5'):\n",
        "        featnames = glob.glob(os.path.join(sample_name, '*.' + feattype))\n",
        "        featnames.sort()\n",
        "        feat = []\n",
        "        for name in featnames:\n",
        "            feat.append(caffeFeatureLoader.loadC3DFeature(name)[0])\n",
        "        return feat\n",
        "\n",
        "    @staticmethod\n",
        "    def loadC3DFeature(filename):\n",
        "        feat = []\n",
        "        with open(filename, 'rb') as fileData:\n",
        "            num = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            chanel = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            length = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            height = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            width = struct.unpack(\"i\", fileData.read(4))[0]\n",
        "            blob_shape = [num, chanel, length, height, width]\n",
        "            m = num * chanel * length * height * width\n",
        "            for i in range(m):\n",
        "                val = struct.unpack(\"f\", fileData.read(4))[0]\n",
        "                feat.append(val)\n",
        "        return feat, blob_shape\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcNkQZLZ7Gcw"
      },
      "source": [
        "class Environment():\n",
        "    def __init__(self, ph_dataset ,dataLoader, window_size = 8, skip = 4):\n",
        "      self.ph_dataset = ph_dataset\n",
        "      self.dataLoader = dataLoader\n",
        "      self.window_size = window_size\n",
        "      self.skip = skip\n",
        "      self.current_video = None\n",
        "      self.sample = None\n",
        "      self.gen = None\n",
        "      self.actions = []\n",
        "      self.num_vids = len(self.dataLoader)\n",
        "    \n",
        "    def append_a(self, actions, a):\n",
        "      if len(actions) == 0:\n",
        "        actions = a\n",
        "      else:\n",
        "        np.append(actions, a[int(len(a)/2):])\n",
        "      actions = np.array(actions).flatten()\n",
        "      return actions     \n",
        "\n",
        "\n",
        "    def next_video(self):\n",
        "      self.sample = next(iter(self.dataLoader))\n",
        "      #self.current_video, self.label, _ = next(dataLoader)\n",
        "      self.gen = self.gen_data_and_gloss()\n",
        "      #frames, _, _ = self.step()\n",
        "      data,_,_ = self.step()   \n",
        "      return data   #what do we want to return here?\n",
        "\n",
        "    def gen_data_and_gloss(self):\n",
        "      sample = self.sample\n",
        "      for i in range(sample['data'].shape[1]):\n",
        "        # print(sample)\n",
        "        yield {'data': sample['data'][:,i,:], 'gloss': sample['gloss'][:,i,:], 'gloss_text': sample['gloss_text'][i]}\n",
        "\n",
        "\n",
        "    # def gen_frames_and_reward(self):\n",
        "    #   video = self.current_video\n",
        "\n",
        "    #   if video.shape[0] % self.window_size:\n",
        "    #     raise Exception(\"Video size not multiple of window size!\")\n",
        "    #     return\n",
        "\n",
        "    #   for i in range(0,video.shape[0] - self.window_size + 1, self.skip): \n",
        "    #       reward = 0\n",
        "    #       done = False\n",
        "    #       frames = video[i:i+self.window_size,:,:,:]\n",
        "    #       if(i >= video.shape[0] - self.window_size): #is last set of frames\n",
        "    #         self.hyp = action_to_words(self.actions)\n",
        "    #         reward = get_wer_delsubins(self.ref, self.hyp)\n",
        "    #         done = True\n",
        "    #       yield frames, reward, done\n",
        "\n",
        "    def get_wer(self, ref,actions):\n",
        "      un_tags = np.where(np.logical_or(actions == self.ph_dataset.voc.word2index['<EOS>'], actions == self.ph_dataset.voc.word2index['<BOS>']))\n",
        "      actions = np.delete(actions, un_tags)\n",
        "      hyp_list = [i[0] for i in groupby(self.actions)]\n",
        "      print(ref,hyp_list)\n",
        "      ref = ' '.join(e for e in self.ph_dataset.index2sentence(ref.numpy()[0]))\n",
        "      print('ref: ',ref)\n",
        "      hyp = ' '.join(e for e in self.ph_dataset.index2sentence(hyp_list))\n",
        "      wer = get_wer_delsubins(ref, hyp)\n",
        "      return 1 - wer[0]\n",
        "\n",
        "    def step(self,action = None):\n",
        "      done = False\n",
        "      reward = 0\n",
        "      # CHANGE: step returns (data:(1,512), gloss:(1,10))\n",
        "      if action:\n",
        "        self.actions = self.append_a(self.actions, action)\n",
        "      #frames, reward, done = next(self.gen, ('', '', ''))\n",
        "      clip = next(iter(self.gen), None)\n",
        "     \n",
        "      if clip == None:\n",
        "        print('last clip')\n",
        "        done = True\n",
        "        self.ref = self.sample['label']\n",
        "        # hyp_list = [i[0] for i in groupby(self.actions)] \n",
        "        # self.hyp = ' '.join([str(e) for e in hyp_list])\n",
        "        reward = self.get_wer(self.ref, self.actions)\n",
        "        \n",
        "      return clip, reward, done\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkHC2XPadpvY"
      },
      "source": [
        "# BASE_DIR = \"/content/drive/MyDrive/RL_Project_CodeBase\"\n",
        "# PHOE_DIR = os.path.join(BASE_DIR,'phoenix2014-release/phoenix-2014-multisigner')\n",
        "# FEAT_DIR = os.path.join(BASE_DIR,'c3d_res_phoenix_body_iter5_120k')\n",
        "# ANNOT_DIR = os.path.join(PHOE_DIR,'annotations')\n",
        "# CORPUS_DIR = os.path.join(ANNOT_DIR,'manual')\n",
        "# video_dataset = PhoenixVideo(CORPUS_DIR, ANNOT_DIR, FEAT_DIR, 'train' )\n",
        "# video_dataset.__len__()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k43uXYVzBTLH"
      },
      "source": [
        "def get_wer_delsubins(ref, hyp, debug=False):\n",
        "    DEL_PENALTY = 1\n",
        "    SUB_PENALTY = 1\n",
        "    INS_PENALTY = 1\n",
        "    r = ref\n",
        "    h = hyp\n",
        "    # costs will holds the costs, like in the Levenshtein distance algorithm\n",
        "    costs = [[0 for inner in range(len(h) + 1)] for outer in range(len(r) + 1)]\n",
        "    # backtrace will hold the operations we've done.\n",
        "    # so we could later backtrace, like the WER algorithm requires us to.\n",
        "    backtrace = [[0 for inner in range(len(h) + 1)] for outer in range(len(r) + 1)]\n",
        "\n",
        "    OP_OK = 0\n",
        "    OP_SUB = 1\n",
        "    OP_INS = 2\n",
        "    OP_DEL = 3\n",
        "\n",
        "    # First column represents the case where we achieve zero\n",
        "    # hypothesis words by deleting all reference words.\n",
        "    for i in range(1, len(r) + 1):\n",
        "        costs[i][0] = DEL_PENALTY * i\n",
        "        backtrace[i][0] = OP_DEL\n",
        "\n",
        "    # First row represents the case where we achieve the hypothesis\n",
        "    # by inserting all hypothesis words into a zero-length reference.\n",
        "    for j in range(1, len(h) + 1):\n",
        "        costs[0][j] = INS_PENALTY * j\n",
        "        backtrace[0][j] = OP_INS\n",
        "\n",
        "    # computation\n",
        "    for i in range(1, len(r) + 1):\n",
        "        for j in range(1, len(h) + 1):\n",
        "            if r[i - 1] == h[j - 1]:\n",
        "                costs[i][j] = costs[i - 1][j - 1]\n",
        "                backtrace[i][j] = OP_OK\n",
        "            else:\n",
        "                substitutionCost = costs[i - 1][j - 1] + SUB_PENALTY  # penalty is always 1\n",
        "                insertionCost = costs[i][j - 1] + INS_PENALTY  # penalty is always 1\n",
        "                deletionCost = costs[i - 1][j] + DEL_PENALTY  # penalty is always 1\n",
        "\n",
        "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
        "                if costs[i][j] == substitutionCost:\n",
        "                    backtrace[i][j] = OP_SUB\n",
        "                elif costs[i][j] == insertionCost:\n",
        "                    backtrace[i][j] = OP_INS\n",
        "                else:\n",
        "                    backtrace[i][j] = OP_DEL\n",
        "\n",
        "    # back trace though the best route:\n",
        "    i = len(r)\n",
        "    j = len(h)\n",
        "    numSub = 0\n",
        "    numDel = 0\n",
        "    numIns = 0\n",
        "    numCor = 0\n",
        "    if debug:\n",
        "        print(\"OP\\tREF\\tHYP\")\n",
        "        lines = []\n",
        "    while i > 0 or j > 0:\n",
        "        if backtrace[i][j] == OP_OK:\n",
        "            numCor += 1\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "            if debug:\n",
        "                lines.append(\"OK\\t\" + r[i] + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_SUB:\n",
        "            numSub += 1\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "            if debug:\n",
        "                lines.append(\"SUB\\t\" + r[i] + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_INS:\n",
        "            numIns += 1\n",
        "            j -= 1\n",
        "            if debug:\n",
        "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_DEL:\n",
        "            numDel += 1\n",
        "            i -= 1\n",
        "            if debug:\n",
        "                lines.append(\"DEL\\t\" + r[i] + \"\\t\" + \"****\")\n",
        "    if debug:\n",
        "        lines = reversed(lines)\n",
        "        for line in lines:\n",
        "            print(line)\n",
        "        print(\"#cor \" + str(numCor))\n",
        "        print(\"#sub \" + str(numSub))\n",
        "        print(\"#del \" + str(numDel))\n",
        "        print(\"#ins \" + str(numIns))\n",
        "    return (numSub + numDel + numIns) / (float)(len(r)), numSub / float(len(r)), numIns / float(len(r)), numDel / float(len(r))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jZqshlkmXJR",
        "outputId": "89fad9df-a278-4fcf-914c-4f435cae3a9e"
      },
      "source": [
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/RL_Project_CodeBase\"\n",
        "PHOE_DIR = os.path.join(BASE_DIR,'phoenix2014-release/phoenix-2014-multisigner')\n",
        "FEAT_DIR = os.path.join(BASE_DIR,'c3d_res_phoenix_body_iter5_120k')\n",
        "ANNOT_DIR = os.path.join(PHOE_DIR,'annotations')\n",
        "CORPUS_DIR = os.path.join(ANNOT_DIR,'manual')\n",
        "\n",
        "\n",
        "# env = ENV()\n",
        "# helper function append_action(actions, a): returns appended action after ignoring 4 overlap frames\n",
        "video_dataset = PhoenixVideo(CORPUS_DIR, ANNOT_DIR, FEAT_DIR, 'train' )\n",
        "dataLoader = DataLoader(video_dataset,\n",
        "    batch_size=1, shuffle=False)\n",
        "\n",
        "env = Environment(video_dataset, dataLoader)\n",
        "env2 = Environment(video_dataset, dataLoader)\n",
        "opts = ED({\n",
        "    \"src_vocab_size\": 512,\n",
        "    \"trg_vocab_size\": video_dataset.voc.num_words,\n",
        "    \"src_pad_idx\": 0,\n",
        "    \"trg_pad_idx\": 0,\n",
        "    \"embed_size\" : 512,\n",
        "    \"num_layers\": 6,\n",
        "    \"forward_expansion\": 4,\n",
        "    \"heads\": 8,\n",
        "    \"dropout\": 0.3,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
        "    \"max_len\": 10,\n",
        "})\n",
        "\n",
        "agent = SC_REINFORCE(opts) # add args \n",
        "train_wers = []\n",
        "dev_wers = []\n",
        "test_wers = []\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src_vocab_size': 512, 'trg_vocab_size': 1237, 'src_pad_idx': 0, 'trg_pad_idx': 0, 'embed_size': 512, 'num_layers': 6, 'forward_expansion': 4, 'heads': 8, 'dropout': 0.3, 'device': 'cpu', 'max_len': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93TJNDwMPI5L"
      },
      "source": [
        "\n",
        "# x = torch.tensor(np.random.randn(1,512))\n",
        "# trg = torch.tensor([np.arange(10)]*1)\n",
        "\n",
        "# print(np.shape(trg))\n",
        "# trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "# src_pad_idx = 0\n",
        "# trg_pad_idx = 0\n",
        "# src_vocab_size = 10\n",
        "# trg_vocab_size = 10\n",
        "# model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
        "#     device\n",
        "# )\n",
        "# out = model(x, trg[:, :-1])\n",
        "# print(np.shape(out))\n",
        "for vid_num in range(env.num_vids): #env.num_epochs\n",
        "  state = env.next_video()\n",
        "  state2 = env2.next_video()\n",
        "  # state = {'clip': x, 'label': trg}\n",
        "  # vid = [state]\n",
        "\n",
        "  reward_s = []\n",
        "  reward_a = []\n",
        "  log_probs = []\n",
        "  done_s = done_a = False\n",
        "\n",
        "  while not done_s or not done_a:\n",
        "    \n",
        "    if not done_s:\n",
        "      a_s, log_prob = agent.select_action_sample(state)\n",
        "        # done_s = True #debug - remove later, env step\n",
        "        # r_s = [1*9] #debug - remove later, env step\n",
        "        #state, reward, done = env.step(action)\n",
        "      print(\"a_s: \",a_s)\n",
        "      state, r_s, done_s = env.step(a_s)\n",
        "      reward_s.append(r_s)\n",
        "      log_probs.append(log_prob)\n",
        "   \n",
        "    if not done_a:\n",
        "      a_a = agent.select_action_argmax(state2)\n",
        "        # done_a = True #debug - remove later, env step\n",
        "        # r_a = [1*10] #debug - remove later, env step\n",
        "      state2, r_a, done_a = env2.step(a_a)\n",
        "      reward_a.append(r_a)\n",
        "\n",
        "    if done_a and done_s:\n",
        "      break\n",
        "      \n",
        "  reward_a, reward_s, log_probs = np.array(reward_a), np.array(reward_s), np.array(log_probs)\n",
        "  agent.update_parameters(reward_s, reward_a,log_probs=log_probs)\n",
        "\n",
        "\n",
        "# agent.inference(state)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWv7AWLRH6g1"
      },
      "source": [
        "with torch.no_grad():\n",
        "  agent.inference(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzsdJgdUUkYe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}