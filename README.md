# CSLR_with_RL
Using RL for CSLR (CSF441 Course Project)

The aim of Continuous Sign Language Recognition (CSLR) is to **translate videos of signlanguage  into  text  sentences**.   CSLR  is  a  fairly  challenging  task,  and  is  currently  anactively  growing  field.   Any  CSLR  model  must  capture  all  the  fine  details  in  hand  move-ments and accurately translate them into coherent sentences without semantic gaps.  Thistask of automatic translation is pertinent in today’s world, considering the communicationchallenges that exist between hearing-impaired individuals and those with hearing abilities.Hence, the use of machines for this task can be particularly beneficial.

The paper we implemented was ”Continuous Sign Language Recognition Via ReinforcementLearning”. It proposes a novel approach to the task of Sign Language Recognition whichapplies Reinforcement Learning.  The CSLR model proposed in this paper consists firstly of a3D ResNet (a type of 3D Convolutional Neural Network) to extract the visual features fromthe videos.  Convolutional Neural Networks are the industry standard for video recognitiontasks.The second portion of the model is a Transformer for the purpose of sign language recogni-tion.  The Transformer is a state of the art model that helps translate the sign videos intotext  sentences.   Transformers  have  shown  tremendous  capabilities  in  machine  translationtasks, which are very similar to the CSLR task that we sought to implement.The Transformer was trained using the **self-critic REINFORCE algorithm**.  Using RLfor training addresses most drawbacks that were observed while using traditional SupervisedLearning techniques.  This RL based optimization strategy is hypothesized to lead to majorimprovements in results
